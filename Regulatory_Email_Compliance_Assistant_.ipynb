{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttz9fKuK6t25"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRB4c4oO7BNP"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload() #Upload FCA COBS 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5Xcf4_K7Hmn"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Replace the name below with the exact uploaded filename\n",
    "local_path = \"/content/COBS 4 Communicating with clients, including financial promotions.pdf\"\n",
    "drive_path = \"/content/drive/MyDrive/FCA_Project/COBS_4.pdf\"   # simpler name\n",
    "\n",
    "# Create folder if it doesn‚Äôt exist\n",
    "import os\n",
    "os.makedirs(\"/content/drive/MyDrive/FCA_Project\", exist_ok=True)\n",
    "\n",
    "# Move file\n",
    "shutil.move(local_path, drive_path)\n",
    "\n",
    "print(\"Saved to Drive at:\", drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5Z-sJlX7NAG"
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Use the path where you saved the file\n",
    "pdf_path = \"/content/drive/MyDrive/FCA_Project/COBS_4.pdf\"\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# Split into pages\n",
    "pages = loader.load()\n",
    "print(f\"Total pages loaded: {len(pages)}\")\n",
    "\n",
    "print(pages[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO2knyBHDguX"
   },
   "outputs": [],
   "source": [
    "# Section-aware safe chunking + Embeddings + FAISS\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# --- Reload tokenizer for MiniLM ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Merge pages into one big text\n",
    "full_text = \"\\n\".join([p.page_content for p in pages])\n",
    "\n",
    "# Regex split into sections by headers\n",
    "sections = re.split(r\"(COBS\\s+4\\.\\d+[A-Z]?\\s+.*?)\", full_text)\n",
    "\n",
    "docs = []\n",
    "for i in range(1, len(sections), 2):\n",
    "    heading = sections[i].strip()\n",
    "    body = sections[i+1].strip() if (i+1) < len(sections) else \"\"\n",
    "    text = heading + \"\\n\" + body\n",
    "    docs.append(Document(page_content=text, metadata={\"section\": heading}))\n",
    "\n",
    "print(f\"Initial sections: {len(docs)}\")\n",
    "\n",
    "# Sub-chunker for large sections\n",
    "sub_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,   # safe under 512 tokens\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "final_docs = []\n",
    "for doc in docs:\n",
    "    tokens = len(tokenizer.encode(doc.page_content))\n",
    "    if tokens > 512:\n",
    "        # Split large sections into smaller chunks\n",
    "        sub_chunks = sub_splitter.split_text(doc.page_content)\n",
    "        for chunk in sub_chunks:\n",
    "            final_docs.append(Document(page_content=chunk, metadata={\"section\": doc.metadata[\"section\"]}))\n",
    "    else:\n",
    "        final_docs.append(doc)\n",
    "\n",
    "print(f\"Final chunks created: {len(final_docs)}\")\n",
    "\n",
    "# Create embeddings + FAISS index\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "faiss_index = FAISS.from_documents(final_docs, embedding_model)\n",
    "\n",
    "# Save FAISS index to Drive\n",
    "save_path = \"/content/drive/MyDrive/FCA_Project/faiss_index\"\n",
    "faiss_index.save_local(save_path)\n",
    "\n",
    "print(f\"‚úÖ FAISS index created and saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F-jjwCzE0Ev"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Reload the FAISS index from Drive\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.load_local(\n",
    "    \"/content/drive/MyDrive/FCA_Project/faiss_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Inspect a few documents\n",
    "docs = faiss_index.similarity_search(\"misleading promotions\", k=3)\n",
    "\n",
    "for i, d in enumerate(docs, start=1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(\"Section:\", d.metadata[\"section\"])\n",
    "    print(\"Preview:\", d.page_content[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVyAq9YwK_yC"
   },
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_Uyxn7ydJNT",
    "outputId": "539e59fb-d217-4180-fbb8-b5adb4432730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMhd0Kg-HOJI"
   },
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub transformers accelerate langchain langchain-community faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ski44fJ1dHQa"
   },
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# üîë Replace with your own HF token\n",
    "login(\"HF_Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32vLHTwaWOm6"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Use the same MiniLM embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "faiss_index = FAISS.load_local(\n",
    "    \"/content/drive/MyDrive/FCA_Project/faiss_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn7gu2x8WSq5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL-1m5D8W1lS"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, pipeline_kwargs={\"return_full_text\": False})\n",
    "\n",
    "retriever = faiss_index.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,       # number of final results\n",
    "        \"fetch_k\": 15 # wider pool before filtering for diversity\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yPM0FjgCqt5"
   },
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxSqnx_XCp2d"
   },
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are an FCA compliance assistant.\n",
    "You will receive FCA Handbook extracts (COBS 4 only) as CONTEXT and an EMAIL.\n",
    "Your job is to decide if the EMAIL complies with the CONTEXT.\n",
    "\n",
    "If you are not sure from the CONTEXT, your decision must be \"Insufficient context\".\n",
    "\n",
    "Allowed decisions:\n",
    "- \"Compliant\"\n",
    "- \"Not Compliant\"\n",
    "- \"Insufficient context\"\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the CONTEXT; do not rely on outside knowledge.\n",
    "- Cite the specific COBS 4 sections you used (e.g., \"COBS 4.2\").\n",
    "- Rewrite the EMAIL only if your decision is \"Not Compliant\".\n",
    "- If decision is \"Compliant\" or \"Insufficient context\", the \"email\" field must be \"\".\n",
    "- Keep answers short, professional, and JSON only (no explanations outside JSON).\n",
    "\n",
    "‚ö†Ô∏è Rewriting rules:\n",
    "- Preserve ALL factual details from the EMAIL (numbers, percentages, dates, names, descriptors like \"low-risk\").\n",
    "- Do NOT invent, add, or paraphrase disclaimers, warnings, or risk statements unless the exact wording appears verbatim in CONTEXT.\n",
    "- If CONTEXT provides mandatory disclaimer wording verbatim, insert it exactly as written (no changes).\n",
    "- You must NEVER change descriptors (e.g., do not replace \"low-risk\" with \"high-risk\").\n",
    "- You must NEVER keep absolute guarantee terms like \"guaranteed\", \"guarantee\", \"no risk\", \"risk-free\", or \"assured\". Replace them with neutral alternatives such as \"offers\", \"may\", or \"potential\". If no compliant rewrite is possible, remove the offending part entirely.\n",
    "- Only remove or rephrase wording that is misleading or prohibited by the cited COBS 4 section.\n",
    "- Keep the rewrite as short and as close as possible to the original EMAIL.\n",
    "\n",
    "‚ö†Ô∏è Strict formatting rule:\n",
    "Return ONLY ONE JSON object.\n",
    "Do not provide multiple alternatives.\n",
    "Do not repeat decisions.\n",
    "\n",
    "Return your answer strictly in this format:\n",
    "\n",
    "<JSON>\n",
    "{{\n",
    "  \"decision\": \"Compliant\" | \"Not Compliant\" | \"Insufficient context\",\n",
    "  \"sections\": [\"COBS 4.x\", ...],\n",
    "  \"email\": \"Rewritten email if decision is 'Not Compliant', otherwise empty string\"\n",
    "}}\n",
    "</JSON>\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "EMAIL:\n",
    "{question}\n",
    "\n",
    "‚ö†Ô∏è Output ONLY one JSON object. Do not add explanations, labels, or extra text.\n",
    "Begin directly with <JSON> and end with </JSON>.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Optional sanity check:\n",
    "print(\"Prompt input vars:\", prompt.input_variables)  # should be ['context', 'question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ll9z316rROK5"
   },
   "outputs": [],
   "source": [
    "# --- Test query (non-compliant email) ---\n",
    "email_text = \"Our product gaurantees 20% returns with zero risk. Sign up today! Best regards, [Your Name]\"\n",
    "result = qa.invoke({\"query\": email_text})\n",
    "\n",
    "# --- Post-process model output into JSON ---\n",
    "import json\n",
    "import re\n",
    "\n",
    "raw_output = result[\"result\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq8QJrF9fHU7"
   },
   "outputs": [],
   "source": [
    " print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noAFYrXpMOH1"
   },
   "outputs": [],
   "source": [
    "matches = re.findall(r\"<JSON>(.*?)</JSON>\", raw_output, re.DOTALL)\n",
    "\n",
    "if not matches:\n",
    "    print(\"‚ö†Ô∏è No JSON found. Raw output:\\n\", raw_output)\n",
    "else:\n",
    "    try:\n",
    "        # Take only the first valid block\n",
    "        clean_json = matches[0].strip()\n",
    "        out = json.loads(clean_json)\n",
    "        print(\"‚úÖ Parsed JSON:\\n\", out)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"‚ö†Ô∏è JSON parse error:\", e)\n",
    "        print(\"Candidate:\\n\", clean_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjGg6lVjTkZf"
   },
   "outputs": [],
   "source": [
    "def check_email(email_text):\n",
    "    res = qa.invoke({\"query\": email_text})\n",
    "    matches = re.findall(r\"<JSON>(.*?)</JSON>\", res[\"result\"], re.DOTALL)\n",
    "    if not matches:\n",
    "        return {\"decision\": \"ParseError\", \"email\": \"\"}\n",
    "    return json.loads(matches[0].strip())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
